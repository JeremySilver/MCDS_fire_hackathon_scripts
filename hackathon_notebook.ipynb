{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febb5d59-24f2-43d2-822e-89174cc81cd2",
   "metadata": {},
   "source": [
    "# Hackathon Project: Environmental Data Analysis for Decision-making\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to our hackathon project! In this Jupyter Notebook, we embark on a journey to analyze various environmental datasets to extract meaningful insights. The main objective is to make data-driven decisions related to environmental concerns such as weather patterns, land-use, and hotspot activities.\n",
    "\n",
    "## Datasets Used\n",
    "\n",
    "1. **Alum Classes** - Contains geo-spatial data related to different alum classes.\n",
    "2. **DEM (Digital Elevation Model)** - Provides topographical information.\n",
    "3. **Hotspots** - Includes satellite data about geographical hotspots.\n",
    "4. **MODIS MCD12C1 Land-use Classification** - Contains land-use categories.\n",
    "5. **ERA5 Hourly Weather Data** - Provides hourly weather data.\n",
    "6. **AGCD (Australian Gridded Climate Data)** - Contains daily weather data.\n",
    "7. **BARRA Hourly Weather Data** - Another source of hourly weather data.\n",
    "\n",
    "## Tools and Libraries\n",
    "\n",
    "- GDAL for geo-spatial data manipulation\n",
    "- NetCDF for multi-dimensional data storage\n",
    "- Pandas for data manipulation and analysis\n",
    "- Matplotlib for data visualization\n",
    "- Xarray for handling labeled multi-dimensional arrays\n",
    "\n",
    "## Structure of the Notebook\n",
    "\n",
    "The notebook is structured into several parts, each focusing on a different aspect of environmental data:\n",
    "\n",
    "1. **Data Reading and Pre-processing** - Importing and cleaning data.\n",
    "2. **Data Transformation and Projection** - Converting data into desired formats and projections.\n",
    "3. **Data Filtering and Subsetting** - Narrowing down the datasets based on certain criteria such as Region of Interest (ROI) and time periods.\n",
    "4. **Data Visualization** - Plotting the datasets for visual interpretation.\n",
    "5. **Data Analysis** - Extracting meaningful insights from the data.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b3ea-6e5b-4565-bb25-b718cbf8f2d1",
   "metadata": {},
   "source": [
    "## Hackathon Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a587077-53de-437a-8c09-d0b36dd9e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --target=./packages rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508b3c8a-5e27-4897-8fee-d2d40bc6ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52ce653-3059-48cf-b46d-4ee498652ee5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'osgeo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Importing libraries for geospatial data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ogr\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'osgeo'"
     ]
    }
   ],
   "source": [
    "# Importing libraries for data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray\n",
    "import dask\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import cftime\n",
    "import time\n",
    "import io\n",
    "import osgeo\n",
    "\n",
    "# Importing libraries for geospatial data\n",
    "from osgeo import ogr\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Importing libraries for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Importing library for working with netCDF files\n",
    "import netCDF4 as nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd353ce-967d-4732-b3c3-6c77c76be4a1",
   "metadata": {},
   "source": [
    "## Define Region of Interest (ROI) and Study Period\n",
    "In this section, we define the geographic boundaries and time frame for our study. We have two options for geographic scope: Victoria or Australia as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a51fd-f874-42cf-bf35-999571ab6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If just_roi is True, use Victoria's bounding box, else use Australia's bounding box\n",
    "just_roi = True\n",
    "\n",
    "if just_roi:\n",
    "    # Box around Victoria (coordinates found using https://boundingbox.klokantech.com/)\n",
    "    roi_lon_bounds = [140.53, 150.03]\n",
    "    roi_lat_bounds = [-39.20, -33.73]\n",
    "else:\n",
    "    # Box around Australia\n",
    "    roi_lon_bounds = [106.3, 160.8]\n",
    "    roi_lat_bounds = [-44.2,  -8.9]\n",
    "\n",
    "# Define the study period\n",
    "study_period_bounds = [\n",
    "    pd.Timestamp('2020-01-01T00:00:00', tz='UTC'), \n",
    "    pd.Timestamp('2020-02-01T00:00:00', tz='UTC')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18b016-6c17-434c-9c48-d60e94c85ccc",
   "metadata": {},
   "source": [
    "## Load ALUM Land-Use Data\n",
    "\n",
    "Here, we load the ALUM land-use data from a CSV file and a TIFF file. We use the GDAL library to read the TIFF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a5e51-7b8a-4dd2-a5ed-01677c1912be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata from CSV file\n",
    "alum_metadata = pd.read_csv('data/ALUM/NLUM_ALUMV8_250m_2015_16_alb/NLUM_ALUMV8_250m_2015_16_alb.csv')\n",
    "\n",
    "# Open the TIFF dataset using GDAL\n",
    "alum_dataset = gdal.Open('data/ALUM/NLUM_ALUMV8_250m_2015_16_alb/NLUM_ALUMV8_250m_2015_16_alb.tif', gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the first raster band from the dataset\n",
    "alum_band = alum_dataset.GetRasterBand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf9afc-4218-48f9-adf5-07d38cf36c39",
   "metadata": {},
   "source": [
    "## Extract and Transform Geospatial Metadata\n",
    "In this section, we extract metadata from the ALUM land-use dataset. We also set up coordinate transformations to align the dataset with our Region of Interest (ROI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385a3cc-4b45-49a4-9f41-006a976595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from the ALUM dataset\n",
    "alum_ulx, alum_xres, alum_xskew, alum_uly, alum_yskew, alum_yres  = alum_dataset.GetGeoTransform()\n",
    "alum_nx = alum_dataset.RasterXSize  # Number of columns\n",
    "alum_ny = alum_dataset.RasterYSize  # Number of rows\n",
    "\n",
    "# Calculate bottom-right x and y coordinates\n",
    "alum_lrx = alum_ulx + (alum_nx * alum_xres)\n",
    "alum_lry = alum_uly + (alum_ny * alum_yres)\n",
    "\n",
    "# Get the projection details\n",
    "alum_proj = alum_dataset.GetProjection()\n",
    "\n",
    "# Initialize source and target spatial references\n",
    "source = osr.SpatialReference()\n",
    "source.ImportFromWkt(alum_proj)  # Import from well-known text\n",
    "\n",
    "target = osr.SpatialReference()\n",
    "target.ImportFromEPSG(4326)  # WGS84\n",
    "\n",
    "# Create coordinate transformations\n",
    "transform = osr.CoordinateTransformation(source, target)\n",
    "invtransform = osr.CoordinateTransformation(target, source)\n",
    "\n",
    "# Transform the upper-left corner point to lat, lon for testing\n",
    "alum_ulx_lat, alum_ulx_lon, _ = transform.TransformPoint(alum_ulx, alum_uly)\n",
    "\n",
    "# Calculate the x and y coordinates for each pixel\n",
    "alum_xrow = alum_ulx + np.arange(alum_nx, dtype=np.float) * alum_xres\n",
    "alum_ycol = alum_uly + np.arange(alum_ny, dtype=np.float) * alum_yres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00254b1e-f1a5-4dc1-8841-7e27e1ed4b55",
   "metadata": {},
   "source": [
    "## Calculate ROI in Dataset Coordinates\n",
    "\n",
    "We need to translate our geographic ROI into pixel coordinates that align with the ALUM dataset. This involves several transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed11fc7-43a7-42bc-b03a-215fee80c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ROI lat, lon to pixel coordinates\n",
    "roi_xy_points = []\n",
    "for ix in range(0, 2):\n",
    "    for iy in range(0, 2):\n",
    "        px, py, _ = invtransform.TransformPoint(roi_lat_bounds[ix], roi_lon_bounds[iy])\n",
    "        roi_xy_points.append([px, py])\n",
    "\n",
    "# Extract the bounds for ROI in dataset coordinates\n",
    "roi_xy_points_np = np.array(roi_xy_points)\n",
    "roi_xlim = np.array([roi_xy_points_np[:, 0].min(), roi_xy_points_np[:, 0].max()])\n",
    "roi_ylim = np.array([roi_xy_points_np[:, 1].min(), roi_xy_points_np[:, 1].max()])\n",
    "roi_ixlim = np.searchsorted(alum_xrow, roi_xlim)\n",
    "roi_iylim = alum_ny - np.searchsorted(alum_ycol[::-1], roi_ylim)[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335466cb-c29e-4c6d-8817-b59bd1e96dee",
   "metadata": {},
   "source": [
    "## Extract and Visualize ROI from Dataset\n",
    "\n",
    "Finally, we extract the part of the ALUM dataset that corresponds to our ROI and visualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2232b4-f0cd-4cf1-83d7-210b004f29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the array data for ROI\n",
    "alum_roi_nx = int(roi_ixlim[1] - roi_ixlim[0])\n",
    "alum_roi_ny = int(roi_iylim[1] - roi_iylim[0])\n",
    "alum_roi_arr = alum_band.ReadAsArray(int(roi_ixlim[0]), int(roi_iylim[0]), alum_roi_nx, alum_roi_ny)\n",
    "\n",
    "# Plot and save the image\n",
    "res = plt.imshow(alum_roi_arr, interpolation='nearest')\n",
    "plt.savefig(\"alum_classes_roi.png\")\n",
    "plt.close()\n",
    "\n",
    "# Calculate x and y coordinates for each pixel in the ROI\n",
    "alum_roi_xrow = alum_xrow[roi_ixlim[0]] + np.arange(alum_roi_nx, dtype=np.float) * alum_xres\n",
    "alum_roi_ycol = alum_ycol[roi_iylim[0]] + np.arange(alum_roi_ny, dtype=np.float) * alum_yres\n",
    "\n",
    "# Generate 2D arrays for x and y coordinates\n",
    "alum_roi_x = np.tile(alum_roi_xrow.reshape((alum_roi_nx, 1)), alum_roi_ny)\n",
    "alum_roi_y = np.tile(alum_roi_ycol.reshape((alum_roi_ny, 1)), alum_roi_nx).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a60907-3c53-4fdc-ad5b-2fef5864f68b",
   "metadata": {},
   "source": [
    "## Coordinate Transformation for ROI\n",
    "\n",
    "In this section, we apply coordinate transformations to the extracted Region of Interest (ROI) from the ALUM dataset. This allows us to convert the dataset's native coordinates into latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39502eb8-8ea8-46d3-8f15-88abdac51611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply coordinate transformations to ROI\n",
    "# Note: This operation can be slow, especially for larger regions\n",
    "f = lambda x, y: transform.TransformPoint(x, y)\n",
    "vf = np.vectorize(f)\n",
    "alum_roi_lat, alum_roi_lon, _ = vf(alum_roi_x, alum_roi_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d467e7-3538-4978-9994-41480bedfa90",
   "metadata": {},
   "source": [
    "## Load and Transform DEM Data\n",
    "\n",
    "Here, we load the Digital Elevation Model (DEM) data from a NetCDF file. We also extract the region corresponding to our previously defined ROI and adjust the data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c86e3-3d60-4e88-a2af-2c967b081324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEM data from NetCDF file\n",
    "dem_dataset = nc.Dataset('data/DEM/dem-9s.nc')\n",
    "dem_lon = dem_dataset.variables['lon'][:].data\n",
    "dem_lat = dem_dataset.variables['lat'][:].data\n",
    "\n",
    "# Find the DEM indices corresponding to the ROI\n",
    "roi_ixlim = np.searchsorted(dem_lon, roi_lon_bounds)\n",
    "roi_iylim = np.searchsorted(dem_lat, roi_lat_bounds)\n",
    "\n",
    "# Extract DEM data for ROI\n",
    "dem_roi_arr = dem_dataset.variables['Band1'][roi_iylim[0]:roi_iylim[1], roi_ixlim[0]:roi_ixlim[1]][:].data\n",
    "dem_dataset.close()\n",
    "\n",
    "# Flip the vertical orientation for plotting\n",
    "dem_roi_arr = dem_roi_arr[::-1, :]\n",
    "\n",
    "# Plot and save the DEM ROI including land and sea\n",
    "res = plt.imshow(dem_roi_arr, interpolation='nearest')\n",
    "plt.savefig(\"dem_roi_landsea.png\")\n",
    "plt.close()\n",
    "\n",
    "# Replace ocean points (negative values) with zero for better visualization\n",
    "dem_roi_arr[dem_roi_arr < -100] = 0.0\n",
    "\n",
    "# Plot and save the modified DEM ROI\n",
    "res = plt.imshow(dem_roi_arr, interpolation='nearest')\n",
    "plt.savefig(\"dem_roi.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbf6dc-4622-435e-be8b-b899a310a4e3",
   "metadata": {},
   "source": [
    "## Configure Hotspot Data Reading\n",
    "\n",
    "In this section, we set up the data types for each column in the hotspot dataset. We also prepare for reading the data in chunks to handle its large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa478cb8-d736-4892-92cf-a9a7cabb07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data types for each column in the hotspot dataset\n",
    "dtypes = {\n",
    "    'id': np.int64,\n",
    "    'satellite': np.dtype('O'),\n",
    "    'sensor': np.dtype('O'),\n",
    "    'latitude': np.float32,\n",
    "    'longitude': np.float32,\n",
    "    'temp_kelvin': np.float32,\n",
    "    'power': np.float32,\n",
    "    'confidence': np.float32,\n",
    "    'age_hours': np.int32,\n",
    "    'australian_state': np.dtype('O')\n",
    "}\n",
    "\n",
    "# Initialize variables for chunk-wise reading\n",
    "nrow_total = 31855161  # Total number of rows in the dataset\n",
    "n_per_chunk = 1000000  # Number of rows per chunk\n",
    "n_chunks = int(nrow_total / n_per_chunk) + 1  # Total number of chunks\n",
    "ichunk = 0  # Current chunk index\n",
    "\n",
    "# Open the compressed CSV file for reading\n",
    "f = gzip.open('data/hotspot/all-data.csv.gz', mode='rt')\n",
    "header = f.readline().strip().split(',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d600b37-c873-4bf5-8ec0-bd98d57d313b",
   "metadata": {},
   "source": [
    "## Configure Satellite Filters\n",
    "\n",
    "Here, we define filters to select which satellites' data to use from the hotspot dataset. Users can opt to use all known satellites or a specified subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc6d32-3f83-45a9-a06c-4f10e9677b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define known and used satellites\n",
    "known_satellites = ['AQUA', 'HIMAWARI-8', 'HIMAWARI-9', 'NOAA17', 'NOAA18', 'NOAA19', 'NOAA 19', 'NOAA 20', 'SUOMI NPP', 'TERRA']\n",
    "used_satellites = ['SUOMI NPP', 'AQUA', 'TERRA', 'HIMAWARI-8', 'HIMAWARI-9']  # OR use 'all' to include all known satellites\n",
    "\n",
    "# Initialize a flag for whether to use all satellites\n",
    "all_satellites = False\n",
    "\n",
    "# Validate and set the used_satellites variable\n",
    "if type(used_satellites) == str:\n",
    "    if used_satellites == 'all':\n",
    "        all_satellites = True\n",
    "    else:\n",
    "        if used_satellites in known_satellites:\n",
    "            used_satellites = [used_satellites]\n",
    "        else:\n",
    "            raise RuntimeError(f\"Satellite {used_satellites} not in known types...\")\n",
    "else:\n",
    "    assert type(used_satellites) == list, \"used_satellites should be of 'list' or 'str' type\"\n",
    "    used_satellites = list(set(used_satellites))\n",
    "    assert all([s in known_satellites for s in used_satellites]), \"Some values in 'used_satellites' are not in 'known_satellites'\"\n",
    "    \n",
    "    if len(used_satellites) == len(known_satellites):\n",
    "        all_satellites = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3db59e-1d7c-42cf-a663-36429c3a81de",
   "metadata": {},
   "source": [
    "## Read and Filter Hotspot Data in Chunks\n",
    "\n",
    "In this section, we read the hotspot dataset in chunks since it's a large file. We then apply filters to keep only the relevant rows based on the defined Region of Interest (ROI), study period, and selected satellites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf54fa-d5dd-4dd9-988a-6b94e7c9ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store valid chunks of data\n",
    "valid_chunks = []\n",
    "\n",
    "# Initialize chunk index and read settings\n",
    "ichunk = 0\n",
    "n_chars_per_row = 280  # Estimated characters per row in the file\n",
    "lines_read = 0  # Count of lines read so far\n",
    "\n",
    "# Loop to read data in chunks\n",
    "for ichunk in range(100):  # Using 100 for demonstration; use n_chunks for the entire dataset\n",
    "    print(ichunk)\n",
    "    nrows_this_chunk = n_per_chunk  # Rows to read in this chunk\n",
    "    t0 = time.time()  # Start time\n",
    "\n",
    "    # Read lines for this chunk from the compressed file\n",
    "    hotspot_lines = f.readlines(nrows_this_chunk * n_chars_per_row)\n",
    "    if len(hotspot_lines) == 0:\n",
    "        print('At the end of the file!')\n",
    "        break\n",
    "\n",
    "    t1 = time.time()  # End time\n",
    "    lines_read += len(hotspot_lines)\n",
    "    print(f'\\tThat took {round(t1-t0, 4)} seconds, lines read = {lines_read}')\n",
    "\n",
    "    # Convert the read lines to a DataFrame\n",
    "    hotspot_rows = pd.read_csv(io.StringIO(''.join(hotspot_lines)),\n",
    "                               names=hotspot_colnames,\n",
    "                               header=None,\n",
    "                               usecols=[0, 1, 4, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "                               parse_dates=['datetime'],  # Parsing 'datetime' as actual datetime\n",
    "                               dtype=dtypes)\n",
    "\n",
    "    print(hotspot_rows.shape)\n",
    "    t1 = time.time()\n",
    "    print(f'\\tThat took {round(t1-t0, 4)} seconds')\n",
    "\n",
    "    # Filtering based on the study period\n",
    "    satisfies_time_criteria = (hotspot_rows.datetime > study_period_bounds[0]) & (hotspot_rows.datetime < study_period_bounds[1])\n",
    "    n_times_ok = satisfies_time_criteria.sum()\n",
    "    print(f'\\tNumber satisfying the time criteria = {n_times_ok}')\n",
    "\n",
    "    if n_times_ok == 0:\n",
    "        print(f'\\t\\tSkipping...')\n",
    "        continue\n",
    "\n",
    "    valid_subset = hotspot_rows[satisfies_time_criteria]\n",
    "\n",
    "    # Filtering based on the ROI (Region of Interest)\n",
    "    satisfies_roi_criteria = (valid_subset.longitude > roi_lon_bounds[0]) & (valid_subset.longitude < roi_lon_bounds[1]) & (valid_subset.latitude > roi_lat_bounds[0]) & (valid_subset.latitude < roi_lat_bounds[1])\n",
    "    n_locs_ok = satisfies_roi_criteria.sum()\n",
    "    print(f'\\tNumber satisfying the ROI criteria = {n_locs_ok}')\n",
    "\n",
    "    if n_locs_ok == 0:\n",
    "        print(f'\\t\\tSkipping...')\n",
    "        continue\n",
    "\n",
    "    valid_subset = valid_subset[satisfies_roi_criteria]\n",
    "\n",
    "    # Filtering based on the satellites (if we are not using all satellites)\n",
    "    if not all_satellites:\n",
    "        satisfies_satellite_criteria = valid_subset.satellite.isin(used_satellites)\n",
    "        n_satcrit_ok = satisfies_satellite_criteria.sum()\n",
    "        print(f'\\tNumber satisfying the satellite criteria = {n_satcrit_ok}')\n",
    "\n",
    "        if n_satcrit_ok == 0:\n",
    "            print(f'\\t\\tSkipping...')\n",
    "            continue\n",
    "\n",
    "        valid_subset = valid_subset[satisfies_satellite_criteria]\n",
    "\n",
    "    valid_chunks.append(valid_subset)\n",
    "\n",
    "# Close the compressed file and concatenate all valid chunks\n",
    "f.close()\n",
    "valid_hotspots = pd.concat(valid_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595357d1-846d-4d71-8012-015bc0fe3b2a",
   "metadata": {},
   "source": [
    "## Post-process and Visualize Valid Hotspots\n",
    "\n",
    "After filtering, we sort the valid hotspots by time and satellite. We also visualize them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940c44b-5979-41e8-884d-df609173a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the valid hotspots\n",
    "valid_hotspots = valid_hotspots.sort_values(by=['satellite', 'datetime'])\n",
    "valid_hotspots = valid_hotspots.sort_values(by=['datetime'])\n",
    "\n",
    "# Print the count of hotspots per satellite\n",
    "uniq_sats = valid_hotspots.satellite.unique()\n",
    "for s in uniq_sats:\n",
    "    print(s, 'count =', (valid_hotspots.satellite == s).sum())\n",
    "\n",
    "# Convert to GeoDataFrame for visualization\n",
    "geometry = [Point(xy) for xy in zip(valid_hotspots['longitude'], valid_hotspots['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(valid_hotspots, geometry=geometry)\n",
    "\n",
    "# Plot the hotspots on an Australia map\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "australia = world.loc[world['name'] == 'Australia']\n",
    "gdf.plot(ax=australia.plot(figsize=(10, 6)), marker='o', color='red', markersize=15)\n",
    "\n",
    "# Save the resulting figure\n",
    "plt.savefig('hotspots_mapped.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980a31c-c72a-425b-bbd8-372243f0ac97",
   "metadata": {},
   "source": [
    "## Load and Filter MODIS MCD12C1 Land-Use Classification Data\n",
    "\n",
    "In this section, we load the MODIS MCD12C1 Land-Use Classification data and filter it to keep only the relevant rows based on the defined Region of Interest (ROI) and target date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e7841-1864-45f6-8037-db50ce287e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MODIS MCD12C1 land-use classification dataset\n",
    "luc_mcd12c1_dataset = nc.Dataset('data/LUC/MCD12C1/MCD12C1.061.nc')\n",
    "\n",
    "# Extract latitude, longitude, and date information\n",
    "luc_mcd12c1_lon = luc_mcd12c1_dataset.variables['Longitude'][:].data\n",
    "luc_mcd12c1_lat = luc_mcd12c1_dataset.variables['Latitude'][:].data\n",
    "luc_mcd12c1_date = nc.num2date(luc_mcd12c1_dataset.variables['time'][:].data, luc_mcd12c1_dataset.variables['time'].units)\n",
    "\n",
    "# Select a target date (one index per year)\n",
    "target_luc_mcd12c1_date = cftime.real_datetime(2021, 1, 1, 0, 0, 0)\n",
    "idate_luc_mcd12c1 = [d._to_real_datetime() for d in luc_mcd12c1_date].index(target_luc_mcd12c1_date)\n",
    "\n",
    "# Select the ROI (Region of Interest)\n",
    "roi_ixlim = np.searchsorted(luc_mcd12c1_lon, roi_lon_bounds)\n",
    "roi_iylim = len(luc_mcd12c1_lat) - np.searchsorted(luc_mcd12c1_lat[::-1], roi_lat_bounds)\n",
    "\n",
    "# Extract the relevant data based on ROI and date\n",
    "luc_mcd12c1_roi_arr = luc_mcd12c1_dataset.variables['Majority_Land_Cover_Type_1'][idate_luc_mcd12c1, roi_iylim[1]:roi_iylim[0], roi_ixlim[0]:roi_ixlim[1]].data\n",
    "luc_mcd12c1_dataset.close()\n",
    "\n",
    "# Plot and save the ROI-specific land-use classification\n",
    "res = plt.imshow(luc_mcd12c1_roi_arr, interpolation='nearest')\n",
    "plt.savefig(\"luc_mcd12c1_roi_landsea.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720834f-1e82-42cc-88bc-3e91514fdf21",
   "metadata": {},
   "source": [
    "## Load and Filter ERA5 Hourly Weather Data\n",
    "\n",
    "In this section, we load the ERA5 hourly weather data and filter it to keep only the relevant rows based on the defined Region of Interest (ROI) and study period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b697a36-fad6-4ff7-bdb9-a9973ab6a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available ERA5 variables\n",
    "era5vars = os.listdir('data/weather/era5/australia')\n",
    "\n",
    "# Initialize a dictionary to store the filtered data for each variable\n",
    "era5_data = {}\n",
    "era5_coord_vars = ['latitude', 'longitude', 'time']\n",
    "\n",
    "# Loop through each ERA5 variable to load and filter data\n",
    "for era5var in era5vars:\n",
    "    print(f'Loading ERA5 variable {era5var}')\n",
    "    ds = xarray.open_mfdataset(f'data/weather/era5/australia/{era5var}/2???/*.nc')\n",
    "    era5_lat = ds.variables['latitude'].to_numpy()\n",
    "    era5_lon = ds.variables['longitude'].to_numpy()\n",
    "    era5_time = ds.variables['time'].to_numpy()\n",
    "\n",
    "    # Determine indices for the ROI and study period\n",
    "    roi_ixlim = np.searchsorted(era5_lon, roi_lon_bounds)\n",
    "    roi_iylim = era5_lat.size - np.searchsorted(era5_lat[::-1], roi_lat_bounds[::-1])\n",
    "    roi_itlim = np.searchsorted(era5_time, [t.to_numpy() for t in study_period_bounds])\n",
    "\n",
    "    # Extract the actual variable name from the dataset\n",
    "    era5varname = [vn for vn in list(ds.variables.keys()) if vn not in era5_coord_vars][0]\n",
    "\n",
    "    # Extract and store the relevant data\n",
    "    era5_data[era5var] = ds.variables[era5varname][roi_itlim[0]:roi_itlim[1], roi_iylim[0]:roi_iylim[1], roi_ixlim[0]:roi_ixlim[1]].to_numpy()\n",
    "\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c190a-b83b-41e7-9e19-8c4d5a0262a2",
   "metadata": {},
   "source": [
    "## Load and Filter AGCD Daily Weather Data\n",
    "\n",
    "In this section, we load the AGCD daily weather data and filter it to keep only the relevant rows based on the defined Region of Interest (ROI) and study period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228377a-f8ce-4826-af53-0c5541730d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available AGCD variables\n",
    "agcdvars = os.listdir('data/weather/agcd')\n",
    "\n",
    "# Initialize a dictionary to store the filtered data for each variable\n",
    "agcd_data = {}\n",
    "agcd_coord_vars = ['lat', 'lon', 'time', 'lat_bnds', 'lon_bnds', 'time_bnds', 'crs']\n",
    "\n",
    "# Loop through each AGCD variable to load and filter data\n",
    "for agcdvar in agcdvars:\n",
    "    ds = xarray.open_mfdataset(f'data/weather/agcd/{agcdvar}/*/r005/01day/*.nc')\n",
    "    agcd_lat = ds.variables['lat'].to_numpy()\n",
    "    agcd_lon = ds.variables['lon'].to_numpy()\n",
    "    agcd_time = ds.variables['time'].to_numpy()\n",
    "\n",
    "    # Determine indices for the ROI and study period\n",
    "    roi_ixlim = np.searchsorted(agcd_lon, roi_lon_bounds)\n",
    "    roi_iylim = np.searchsorted(agcd_lat, roi_lat_bounds)\n",
    "    roi_itlim = np.searchsorted(agcd_time, [t.to_numpy() for t in study_period_bounds])\n",
    "\n",
    "    # Extract the actual variable name from the dataset\n",
    "    agcdvarname = [vn for vn in list(ds.variables.keys()) if vn not in agcd_coord_vars][0]\n",
    "\n",
    "    # Extract and store the relevant data\n",
    "    agcd_data[agcdvar] = ds.variables[agcdvarname][roi_itlim[0]:roi_itlim[1], roi_iylim[0]:roi_iylim[1], roi_ixlim[0]:roi_ixlim[1]].to_numpy()\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a7c13-6c45-44d1-b913-352c27b648b2",
   "metadata": {},
   "source": [
    "## BARRA Hourly Weather Data\n",
    "Here, we are loading and processing BARRA hourly weather data. We will read the data, filter it based on the region of interest and study period, and store it for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f291b-4293-42bd-9d80-dc4f27daa6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variable names from the BARRA dataset\n",
    "barravars = [os.path.basename(f).replace('-PT1H-BARRA_R-v1-200001.nc', '').replace('-fc', '').replace('-spec', '').replace('-slv', '') for f in glob.glob('data/weather/barra/*-200001.nc')]\n",
    "barravar = barravars[0]\n",
    "\n",
    "# Initialize empty dictionary to store BARRA data\n",
    "barra_data = {}\n",
    "\n",
    "# Define coordinate variables used in the BARRA dataset\n",
    "barra_coord_vars = ['lat', 'lon', 'time', 'lat_bnds', 'lon_bnds', 'time_bnds', 'crs']\n",
    "\n",
    "# Load and sort the dataset files\n",
    "fls = glob.glob(f'data/weather/barra/{barravar}*.nc')\n",
    "fls.sort()\n",
    "\n",
    "# Open and immediately close a sample dataset to initialize 'ds'\n",
    "ds = xarray.open_mfdataset(fls[:119])  \n",
    "ds.close()\n",
    "\n",
    "# For some specific files, read the time data to ensure consistency\n",
    "# (This section seems to be checking the time data for specific files)\n",
    "fc = nc.Dataset(fls[120])\n",
    "t120 = fc.variables['time'][:].data\n",
    "fc.close()\n",
    "\n",
    "fc = nc.Dataset(fls[121])\n",
    "t121 = fc.variables['time'][:].data\n",
    "fc.close()\n",
    "\n",
    "fc = nc.Dataset(fls[122])\n",
    "t122 = fc.variables['time'][:].data\n",
    "fc.close()\n",
    "\n",
    "fc = nc.Dataset(fls[123])\n",
    "t123 = fc.variables['time'][:].data\n",
    "fc.close()\n",
    "\n",
    "np.all((t120[1:] - t120[:-1]) == 1.)\n",
    "np.all((t121[1:] - t121[:-1]) == 1.)\n",
    "np.all((t122[1:] - t122[:-1]) == 1.)\n",
    "np.all((t123[1:] - t123[:-1]) == 1.)\n",
    "\n",
    "# Loop through all variables in the BARRA dataset and perform data consistency checks\n",
    "for barravar in barravars:\n",
    "    fls = glob.glob(f'data/weather/barra/{barravar}*.nc')\n",
    "    fls.sort()\n",
    "    for fl in fls:\n",
    "        fc = nc.Dataset(fl)\n",
    "        t = fc.variables['time'][:].data\n",
    "        fc.close()\n",
    "        if not np.all((t[1:] - t[:-1]) == 1.):\n",
    "            print(fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929fc80-e575-4afe-93c7-9e102326a23b",
   "metadata": {},
   "source": [
    "## Filter BARRA Data\n",
    "Now, we filter the BARRA data based on the region of interest and study period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca9209-807f-4d02-9ede-79312cba0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinate data\n",
    "barra_lat = ds.variables['lat'].to_numpy()\n",
    "barra_lon = ds.variables['lon'].to_numpy()\n",
    "barra_time = ds.variables['time'].to_numpy()\n",
    "\n",
    "# Find indices within ROI and study period\n",
    "roi_ixlim = np.searchsorted(barra_lon, roi_lon_bounds)\n",
    "roi_iylim = np.searchsorted(barra_lat, roi_lat_bounds)\n",
    "roi_itlim = np.searchsorted(barra_time, [t.to_numpy() for t in study_period_bounds])\n",
    "\n",
    "# Extract data based on the indices\n",
    "barravarname = [vn for vn in list(ds.variables.keys()) if vn not in barra_coord_vars][0]\n",
    "barra_data[barravar] = ds.variables[barravarname][roi_itlim[0]:roi_itlim[1], roi_iylim[0]:roi_iylim[1], roi_ixlim[0]:roi_ixlim[1]].to_numpy()\n",
    "\n",
    "# Subset the time, longitude, and latitude arrays to match the ROI and study period\n",
    "barra_time_subset = barra_time[roi_itlim[0]:roi_itlim[1]]\n",
    "barra_lon_subset = barra_lon[roi_ixlim[0]:roi_ixlim[1]]\n",
    "barra_lat_subset = barra_lat[roi_iylim[0]:roi_iylim[1]]\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493a6c4-f64d-43bf-95de-72c2a84b85fe",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have successfully read, transformed, and filtered multiple environmental datasets to gain insights into various environmental aspects. Through various data visualizations, we've been able to make some preliminary observations that can be valuable for future research and decision-making.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Data Versatility** - Working with multi-dimensional and geo-spatial data.\n",
    "2. **Data Integration** - Successfully integrated different kinds of environmental data.\n",
    "3. **Data Insights** - Extracted preliminary insights that can be valuable for researchers and policymakers.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "While we were able to achieve a lot in this short time, there are some limitations:\n",
    "\n",
    "1. Performance optimization is needed for faster data processing.\n",
    "2. Further in-depth analysis can be conducted to derive more specific insights.\n",
    "\n",
    "## Future Work\n",
    "\n",
    "1. Apply machine learning models to predict future environmental changes based on the data.\n",
    "2. Incorporate more datasets for a more comprehensive analysis.\n",
    "3. Optimize the code for performance, especially the data filtering steps, which are currently time-consuming.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "We would like to thank all the organizers and mentors for their invaluable support and guidance.\n",
    "\n",
    "## Contact Information\n",
    "\n",
    "For any further queries or collaboration, feel free to contact us.\n",
    "\n",
    "Thank you for taking the time to go through our project. We hope you found it insightful!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
